<!DOCTYPE html>
<html>
<title>ACL 2021 Meta Learning for NLP</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="MetaNLP at ACL 2021">
<meta name="twitter:description" content="Meta Learning and Its Applications to Natural Language Processing workshop at ACL 2021">
<meta name="twitter:image:src" content="https://meta-nlp-2021.github.io/images/metanlp.png">
<!--<meta property="og:title" content="MetaNLP at ACL 2021">
<meta property="og:description" content="Meta Learning and Its Applications to Natural Language Processing workshop at ACL 2021">
<meta property="og:image" content="./images/metanlp.png">
<meta property="og:url" content="https://meta-nlp-2021.github.io/">-->
<!--<meta property="og:image" content="./images/acl.png">-->
<link rel="shortcut icon" type="image/x-icon" href="./images/favicon.ico" />
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>
body,h1,h2,h3,h4,h5,h6 {font-family: "Raleway", sans-serif}

body, html {
  height: 100%;
  line-height: 1.8;
}

.crop {
    height:270px;
    width:270px;
    overflow:hidden;
}
.crop img {
  width: 100%;
}

.paragraph_style_7 {
  margin-top: 0px;
  margin-bottom: 0px;
  font-family: 'AvenirNext-Regular', 'Avenir Next';
}

.paragraph_style_8 {
  margin-top: 0px;
  margin-bottom: 0px;
  font-family: 'AvenirNext-Regular', 'Avenir Next';
  font-size: 18px;
  font-weight: bold;
}

.paragraph_style_9 {
  margin-top: 0px;
  margin-bottom: 0px;
  font-family: 'AvenirNext-Regular', 'Avenir Next';
  background: rgb(255, 251, 0);
}


/* Full height image header */
.bgimg-1 {
  background-position: center;
  background-size: cover;
  background-image: url("./images/background.jpeg");
  min-height: 100%;
}

.w3-bar .w3-button {
  padding: 16px;
}

</style>
<body>

<!-- Navbar (sit on top) -->
<div class="w3-top">
  <div class="w3-bar w3-white w3-card" id="myNavbar">
    <a href="#home" class="w3-bar-item w3-button w3-wide">Meta Learning for NLP</a>
    <!-- Right-sided navbar links -->
    <div class="w3-right w3-hide-small">
      <a href="#description" class="w3-bar-item w3-button"><i class="fa fa-info"></i> Description </a>
      <a href="#call" class="w3-bar-item w3-button"><i class="fa fa-file-text"></i> Call for Papers </a>
      <a href="#speakers" class="w3-bar-item w3-button"><i class="fa fa-comment-o	"></i> Speakers </a>
      <a href="#program" class="w3-bar-item w3-button"><i class="fa fa-user-secret"></i> Program </a>
      <a href="#organizer" class="w3-bar-item w3-button"><i class="fa fa-group"></i> Organizers </a>
      <a href="#committee" class="w3-bar-item w3-button"><i class="fa fa-weixin"></i> Program committee </a>
      <a href="#reading" class="w3-bar-item w3-button"><i class="fa fa-book"></i> Reading </a>
      <a href="#contact" class="w3-bar-item w3-button"><i class="fa fa-envelope"></i> Contact </a>
    </div>
    <!-- Hide right-floated links on small screens and replace them with a menu icon -->

    <a href="javascript:void(0)" class="w3-bar-item w3-button w3-right w3-hide-large w3-hide-medium" onclick="w3_open()">
      <i class="fa fa-bars"></i>
    </a>
  </div>
</div>

<!-- Sidebar on small screens when clicking the menu icon -->
<nav class="w3-sidebar w3-bar-block w3-black w3-card w3-animate-left w3-hide-medium w3-hide-large" style="display:none" id="mySidebar">
  <a href="javascript:void(0)" onclick="w3_close()" class="w3-bar-item w3-button w3-large w3-padding-16">Close ×</a>
  <a href="#description" onclick="w3_close()" class="w3-bar-item w3-button">Description</a>
  <a href="#call" onclick="w3_close()" class="w3-bar-item w3-button">Call for Papers</a>
  <a href="#speakers" onclick="w3_close()" class="w3-bar-item w3-button">Speakers</a>
  <a href="#program" onclick="w3_close()" class="w3-bar-item w3-button">Program</a>
  <a href="#organizer" onclick="w3_close()" class="w3-bar-item w3-button">Organizers</a>
  <a href="#committee" onclick="w3_close()" class="w3-bar-item w3-button">Program committee</a>
  <a href="#reading" onclick="w3_close()" class="w3-bar-item w3-button">Reading</a>
  <a href="#contact" onclick="w3_close()" class="w3-bar-item w3-button">Contact</a>
</nav>

<!-- Header with full-height image -->
<header class="bgimg-1 w3-display-container w3-grayscale-min" id="home">
  <div class="w3-display-left w3-text-white" style="padding:48px">
    <span class="w3-jumbo w3-hide-small">Meta Learning and Its Applications to Natural Language Processing</span><br>
    <span class="w3-xxlarge w3-hide-large w3-hide-medium">Meta Learning and Its Applications to Natural Language Processing</span><br>
    <span class="w3-large">Workshop at ACL 2021</span>
    <!--<p><a href="https://neurips-sas-2020.github.io/#organizers" class="w3-button w3-white w3-padding-large w3-large w3-margin-top w3-opacity w3-hover-opacity-off">Link to virtual workshop</a></p> -->
    <p><a href="https://www.softconf.com/acl2021/w16_metanlp21" class="w3-button w3-padding-large w3-large w3-margin-top w3-hover-opacity-off" style="background-color: #f44336;">Submit papers</a></p>
  </div> 
  <!--
  <div class="w3-display-bottomleft w3-text-grey w3-large" style="padding:24px 48px">
    <i class="fa fa-facebook-official w3-hover-opacity"></i>
    <i class="fa fa-instagram w3-hover-opacity"></i>
    <i class="fa fa-snapchat w3-hover-opacity"></i>
    <i class="fa fa-pinterest-p w3-hover-opacity"></i>
    <i class="fa fa-twitter w3-hover-opacity"></i>
    <i class="fa fa-linkedin w3-hover-opacity"></i>
  </div>
  -->
</header>

<!-- Description -->
<div class="w3-row-padding w3-padding-64 w3-container" id="description">
  <div class="w3-content">
      <h3 class="w3-center">Description</h3>
      <p>Deep learning based natural language processing (NLP) has become the mainstream of research in recent years and significantly outperforms conventional methods. However, deep learning models are notorious for being data and computation hungry. These downsides limit such models' application from deployment to different domains, languages, countries, or styles, since collecting in-genre data and model training from scratch are costly. The long-tail nature of human language makes challenges even more significant.
      </p>
      <p>
      Meta-learning, or ‘Learning to Learn’, aims to learn better learning algorithms, including better parameter initialization, optimization strategy, network architecture, distance metrics, and beyond. Meta-learning has been shown to allow faster fine-tuning, converge to better performance, and achieve outstanding results for few-shot learning in many applications. Meta-learning is one of the most important new techniques in machine learning in recent years, but the method is mainly investigated with applications in computer vision. It is believed that meta-learning has excellent potential to be applied in NLP, and some works have been proposed with notable achievements in several relevant problems, e.g., relation extraction, machine translation, and dialogue generation and state tracking. However, it does not catch the same level of attention as in the image processing community.
      </p>
      <p>
      This workshop (Meta Learning and Its Applications to Natural Language Processing Workshop, or MetaNLP) will bring concentrated discussions on meta-learning for the field of NLP via several invited talks, oral and poster sessions with high-quality papers, and a panel of leading researchers from industry and academia. Alongside research work on new meta-learning methods, data, applications, and results, this workshop will call for novel work on understanding, analyzing, and comparing different meta-learning approaches for NLP. The workshop aims to:
      </p>
      <ul>
         <li>Review existing and inspire new meta-learning methods and results
         </li> 
         <li>Motivate the application of meta-learning approaches to more NLP problems in academia and industry, and encourage discussion amongst experts and practitioners from the two realms
         </li>
         <li>Motivate works on comparing different meta-learning methods and comparing meta-learning to other transfer learning methods that have been long utilized for low-resource NLP
         </li>
         <li>Encourage communication within the field of NLP to share knowledge, ideas, and data for meta-learning, and encourage future collaboration to inspire innovation.
         </li>
        </ul> 
  </div>
</div>

<!-- Call -->
<div class="w3-row-padding w3-padding-64 w3-container w3-light-grey" id="call">
  <div class="w3-content">
      <h3 class="w3-center">Call for Papers</h3>
      <p>MetaNLP workshop invites submissions that investigate the theoretical and experimental nature of meta learning methodologies and their applications to NLP. Relevant research directions include, but not limited to:
      </p>
      <ul>
        <li>New meta-learning approaches
        </li> 
        <li>Application of meta-learning models to NLP tasks, such as parsing, dialog system, question answering, summarization, translation
        </li>
        <li>Generalizability of meta-learned models across domains, tasks, or languages
        </li>
        <li>Understanding of why do meta-learning methods work for NLP, for example:    
        </li>
          <ul>
            <li>What does the model learn in meta-learning tasks?
            </li>
            <li>Are there some meta-learning approaches that are suitable for some NLP applications but not others?
            </li>
          </ul>
        <li>Comparative study on meta-learning approaches.
        </li>
      </ul>
      <p>Popular meta-learning topics include, but not limited to:
      </p>
        <ul>
          <li>Learning optimizer
          </li>
          <li>Learning model initialization
          </li>
          <li>Learning metrics or distance measurement
          </li>
          <li>Learning training algorithm
          </li>
          <li>Few shot learning
          </li>
          <li>Network architecture search
          </li>
        </ul>
      <p>We welcome three categories of papers: regular workshop papers, cross-submissions, and extended abstracts. <b>Only the regular workshop paper will be included in the proceedings. The extended Abstracts and cross-submissions will simply be hosted on our websites.</b> Submissions should be made to <a href="https://www.softconf.com/acl2021/w16_metanlp21">softconf</a>.
      </p>
      <ul>
        <li> <b>Regular Workshop Papers</b> The submissions should be in <a target="_blank" rel="noopener noreferrer" href="https://2021.aclweb.org/calls/papers/"style="color:blue;text_decorate=underline">ACL 2021 style</a> between 4 and 8 pages, excluding the references. Authors can add supplementary material in addition to the 8 pages, but reviewers are not required to review the extra material. The papers should present novel research. The review will be double blind and thus all submissions should be anonymized. Double submission is allowed, but the paper accepted by another conference should be moved to cross-submissions.
        </li>
        <li> <b>Extended Abstracts</b> Preliminary but interesting ideas that have not been published before can be submitted as extended abstracts. Ideas and works that would benefit from additional exposure and discussion but are not ready for publication are welcome to be submitted. The submissions should be up to 2 pages long including the references. The review will be double blind and thus all submissions should be anonymized.
        </li>
        <li> <b>Cross-Submissions</b> We also invite works on relevant topics that have appeared in or submitted to alternative venues. Accepted cross-submissions will be presented as posters, with an indication of the original venue. Selection of cross-submissions will be determined solely by the organizing committee.
        </li>
        <li> Accepted papers for all the three tracks will get one additional page to address reviewer comments. </li>
      </ul>
      <div style="padding: 0px 30px 0px">
      <div class="w3-panel w3-Marina w3-border w3-border-blue w3-round">
        <h3><strong>Important Dates</strong></h3>
        <ul>
          <li>Paper Submissions Due: <del>April 26</del><strong style="color: red">&nbsp; May 7, 2021 (AoE)</strong></li>
          <li>Notification of Acceptance: <del>May 28</del><strong style="color: red">&nbsp; May 31, 2021 (AoE)</strong></li>
          <li>Camera-ready Paper Due: June 7, 2021 (AoE)</li>
          <li>Workshop Date: August 5, 2021</li>
        </ul>
      </div>
      </div>
  </div>
</div>

<!-- Speakers -->
<div class="w3-row-padding w3-padding-64 w3-container" id="speakers">
  <div class="w3-content">
  <h3 class="w3-center">Invited Speakers</h3>
  <div class="w3-row-padding" style="margin: auto;">
    <div class="w3-half">
      <div class="w3-card w3-container w3-center" style="">
        <div class="crop" style="margin-left: auto; margin-right: auto;">
        <img  src="./images/vlachos.jpg" style="max-width: 100%; margin-top:10px">
        </div>
        <div class="w3-container">
          <h3> <a target="_blank" rel="noopener noreferrer" href="https://andreasvlachos.github.io">Andreas Vlachos</a></h3>
          <p class="w3-opacity">University of Cambridge</p>
        </div>
      </div>
    </div>
    <div class="w3-half">
      <div class="w3-card w3-container w3-center" style="">
        <div class="crop" style="margin-left: auto; margin-right: auto;">
        <img src="./images/finn.jpg" style="max-width: 100%; margin-top:10px">
        </div>
        <div class="w3-container">
          <h3><a target="_blank" rel="noopener noreferrer" href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a> </h3>
          <p class="w3-opacity">Stanford University</p>
        </div>
      </div>
    </div>
    <div class="w3-half">
      <div class="w3-card w3-container w3-center" style="">
        <div class="crop" style="margin-left: auto; margin-right: auto;">
        <img src="./images/xing.jpg" style="max-width:100%; margin-top:10px">
        </div>
        <div class="w3-container">
          <h3><a target="_blank" rel="noopener noreferrer" href="http://www.cs.cmu.edu/~epxing/">Eric Xing</a></h3>
          <p class="w3-opacity">Carnegie Mellon University</p>
        </div>
      </div>
    </div>
    <div class="w3-half">
      <div class="w3-card w3-container w3-center" style="">
        <div class="crop" style="margin-left: auto; margin-right: auto;">
        <img src="./images/ji.png" style="max-width:100%; margin-top:10px">
        </div>
        <div class="w3-container">
          <h3><a target="_blank" rel="noopener noreferrer" href="http://blender.cs.illinois.edu/hengji.html">Heng Ji</a></h3>
          <p class="w3-opacity">University of Illinois Urbana-Champaign</p>
        </div>
      </div>
    </div>
    <div class="w3-half">
      <div class="w3-card w3-container w3-center" style="">
        <div class="crop" style="margin-left: auto; margin-right: auto;">
        <img src="./images/yu.jpg" style="max-width: 100%; margin-top:10px">
        </div>
        <div class="w3-container">
          <h3><a target="_blank" rel="noopener noreferrer" href="http://zhouyu.cs.ucdavis.edu/">Zhou Yu</a></h3>
          <p class="w3-opacity">Columbia University</p>
        </div>
      </div>
    </div>
  </div>
  </div>
</div>

<!-- Program -->
<div class="w3-row-padding w3-padding-64 w3-container w3-light-grey" id="program">
  <div class="w3-content">
      <h3 class="w3-center">Program</h3>
      <p class="paragraph_style_8">Schedule (EDT)</p>
      <p class="paragraph_style_7">6:00-6:15&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Opening remarks<br></p>
      <p class="paragraph_style_7">6:15-7:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Invited talk - Meta-Learning for few-shot learning in NLP - <strong>Andreas Vlachos</strong><br></p>
      <p class="paragraph_style_7">7:00-7:20&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Contributed talk - Don't Miss the Labels: Label-semantic Augmented Meta-Learner for Few-Shot Text Classification<br></p>
      <p class="paragraph_style_7">7:20-7:40&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Contributed talk - Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent Detection and Slot Filling<br></p>
      <p class="paragraph_style_7">7:40-8:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Contributed talk - Meta-Reinforcement Learning for Mastering Multiple Skills and Generalizing across Environments in Text-based Games<br></p>
      <p class="paragraph_style_7">8:00-8:20&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Contributed talk - Few-Shot Event Detection with Prototypical Amortized Conditional Random Field<br></p>
      <p class="paragraph_style_7">8:20-8:40&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Contributed talk - Meta-Learning for Improving Rare Word Recognition in end-to-end ASR<br></p>
      <p class="paragraph_style_7">8:40-9:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Contributed talk - Minimax and Neyman–Pearson Meta-Learning for Outlier Languages<br></p>
      <p class="paragraph_style_9">9:00-9:15&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Coffee break<br></p>
      <p class="paragraph_style_7">9:15-10:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Invited talk - Meta-learning for dialog systems - <strong>Zhou Yu</strong><br></p>
      <p class="paragraph_style_7">10:00-10:45&nbsp;&nbsp;&nbsp;&nbsp; Invited talk - Learning-to-learn through Model-based Optimization: HPO, NAS, and Distributed Systems - <strong>Eric Xing</strong><br></p>
      <p class="paragraph_style_9">10:45-11:00&nbsp;&nbsp;&nbsp;&nbsp; Coffee break<br></p>
      <p class="paragraph_style_7">11:00-11:20&nbsp;&nbsp;&nbsp;&nbsp; Contributed talk - Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual Transfer<br></p>
      <p class="paragraph_style_7">11:20-11:40&nbsp;&nbsp;&nbsp;&nbsp; Contributed talk - Zero-Shot Compositional Concept Learning<br></p>
      <p class="paragraph_style_7">11:40-12:00&nbsp;&nbsp;&nbsp;&nbsp; Contributed talk - Few Shot Dialogue State Tracking using Meta-learning<br></p>
      <p class="paragraph_style_7">12:00-13:00&nbsp;&nbsp;&nbsp;&nbsp; Poster session<br></p>
      <p class="paragraph_style_9">13:00-13:15&nbsp;&nbsp;&nbsp;&nbsp; Coffee break<br></p>
      <p class="paragraph_style_7">13:15-14:00&nbsp;&nbsp;&nbsp;&nbsp; Invited talk - Few-Shot Learning to Give Feedback in the Real World - <strong>Chelsea Finn</strong><br></p>
      <p class="paragraph_style_7">14:00-14:45&nbsp;&nbsp;&nbsp;&nbsp; Invited talk - Learning from Annotation Guideline: A case study on Event Extraction - <strong>Heng Ji</strong><br></p>
      <p class="paragraph_style_7">14:45-15:00&nbsp;&nbsp;&nbsp;&nbsp; Closing remarks<br></p>

      <p class="paragraph_style_8"><br>Accepted Papers - Talk</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual Transfer</strong></p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Weijia Xu, Batool Haider, Jason Krone and Saab Mansour</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Meta-Reinforcement Learning for Mastering Multiple Skills and Generalizing across Environments in Text-based Games</strong></p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Zhenjie Zhao, Mingfei Sun and Xiaojuan Ma</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Zero-Shot Compositional Concept Learning</strong></p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Guangyue Xu, Parisa Kordjamshidi and Joyce Chai</p>


      <p class="paragraph_style_8"><br>Cross submissions/presentations - Talk</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Meta-Learning for Improving Rare Word Recognition in end-to-end ASR</strong> [ICASSP 2021]</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Florian Lux and Ngoc Thang Vu</p>

      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Few Shot Dialogue State Tracking using Meta-learning</strong> [EACL 2021]</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Saket Dingliwal, Shuyang Gao, Sanchit Agarwal, Chien-Wei Lin, Tagyoung Chung and Dilek Hakkani-Tur</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Few-Shot Event Detection with Prototypical Amortized Conditional Random Field</strong> [ACL 2021 findings]</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Xin Cong, Shiyao Cui, Bowen Yu, Tingwen Liu, Wang Yubin, and Bin Wang</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Minimax and Neyman–Pearson Meta-Learning for Outlier Languages</strong> [ACL 2021 findings]</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Edoardo Maria Ponti, Rahul Aralikatte, Disha Shrivastava, Siva Reddy and Anders Søgaard</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Don't Miss the Labels: Label-semantic Augmented Meta-Learner for Few-Shot Text Classification</strong> [ACL 2021 findings]</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Qiaoyang Luo, Lingqiao Liu, Yuhao Lin, and Wei Emma Zhang</p>

      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent Detection and Slot Filling</strong> [ACL 2021 findings]</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Yutai Hou, Yongkui Lai, Cheng Chen, Wanxiang Che, and Ting Liu</p>


      <p class="paragraph_style_8"><br>Accepted Papers - Posters</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Multi-Pair Text Style Transfer for Unbalanced Data via Task-Adaptive Meta-Learning</strong></p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Xing Han and Jessica Lundin</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>On the cross-lingual transferability of multilingual prototypical models across NLU tasks</strong></p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Oralie Cattan, Sophie Rosset and Christophe Servan</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Meta-Learning for Few-Shot Named Entity Recognition</strong></p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Cyprien de Lichy, Hadrien Glaude and William Campbell</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Multi-accent Speech Separation with One Shot Learning</strong></p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Kuan Po Huang, Yuan-Kuei Wu and Hung-yi Lee</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Semi-supervised Meta-learning for Cross-domain Few-shot Intent Classification</strong></p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Yue Li and Jiong Zhang</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Meta-learning for Classifying Previously Unseen Data Source into Previously Unseen Emotional Categories</strong></p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Gaël Guibon, Matthieu Labeau, Hélène Flamein, Luce Lefeuvre, and Chloé Clavel</p>


      <p class="paragraph_style_8"><br>Accepted extended abstract - Posters</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Meta-learning for Task-oriented Household Text Games</strong></p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Zhenjie Zhao and Xiaojuan Ma</p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;<strong>Meta-learning for downstream aware and agnostic pretraining</strong></p>
      <p class="paragraph_style_7">&nbsp;&nbsp;&nbsp;&nbsp;Hongyin Luo, Shuyan Dong, Yung-Sung Chuang and Shang-Wen Li</p>

  </div>
</div>

<!-- Organizer Section -->

<div class="w3-row-padding w3-padding-64 w3-container" id="organizer"> 
  <div class="w3-content">
  <h3 class="w3-center">Organizers</h3>

  <div class="w3-row w3-light-grey" style="margin-top:10px">
    <div class="w3-col" style="width:150px">
      <img src="./meta-NLP_files/hungyi.jpg" onclick="onClick(this)" alt="Hung-yi Lee received the M.S. and Ph.D. degrees from National Taiwan University (NTU), Taipei, Taiwan, in 2010 and 2012, respectively. From September 2012 to August 2013, he was a postdoctoral fellow in Research Center for Information Technology Innovation, Academia Sinica. From September 2013 to July 2014, he was a visiting scientist at the Spoken Language Systems Group of MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). He is currently an associate professor of the Department of Electrical Engineering of National Taiwan University, with a joint appointment at the Department of Computer Science & Information Engineering of the university. His research focuses on machine learning (especially deep learning), spoken language understanding, and speech recognition. He owns a YouTube channel teaching deep learning in Mandarin (more than 4M Total Views and 48k Subscribers)." class="w3-left w3-circle w3-margin-right w3-hover-opacity" style="width:120px;margin-top:25px;margin-bottom:25px;margin-left:8px">
    </div>
    <div class="w3-rest">
      <p>
        <span class="w3-large w3-text-black w3-margin-right">Hung-Yi Lee</span>
        <p>
        <button class="w3-button w3-light-grey" onclick="window.open('http://speech.ee.ntu.edu.tw/~tlkagk/index.html', '_blank');"><i class="fa fa-user"></i></button>
        <button class="w3-button w3-light-grey" onclick="window.location.href='mailto:hungyilee@ntu.edu.tw';"><i class="fa fa-envelope"></i></button>
        </p>
      </p>
      <p>Associate Professor, National Taiwan University</p>
    </div>
  </div>

  <div class="w3-row" style="margin-top:10px">
    <div class="w3-col" style="width:150px">
      <img src="./meta-NLP_files/mitra.jpg" onclick="onClick(this)" alt="Mitra Mohtarami is a Research Scientist in the Computer Science and Artificial Intelligence Laboratory at MIT. She is a member of MIT's Spoken Language Systems Group, directed by Dr. James Glass. She received her PhD in Computer Science from the National University of Singapore in Dec 2013, and worked as a Research Scientist at the Institute for Infocomm Research (I2R) from Nov 2013 to Aug 2014, and joined MIT as a Postdoctoral Associate in Sep 2014. She has been the recipient of several awards including the Dean's Graduate Research Excellent Award (2013), Outstanding Research Achievement Award (2012), NUS Scholarship Award (2009-2013), and others. Mitra's primary research centers on Natural Language Processing and some of her recent projects include fact checking, open-ended question answering, sentiment and emotion analysis." class="w3-left w3-hover-opacity w3-circle w3-margin-right" style="width:120px; margin-top:25px;margin-bottom:25px;margin-left:8px">
    </div>
    <div class="w3-rest">
      <p><span class="w3-large w3-text-black w3-margin-right">
        Mitra Mohtarami</span>
        <p>
        <button class="w3-button " onclick="window.open('https://www.csail.mit.edu/person/mitra-mohtarami', '_blank');"><i class="fa fa-user"></i></button>
        <button class="w3-button" onclick="window.location.href='mailto:mitra@csail.mit.edu';"><i class="fa fa-envelope"></i></button>
        </p>
      </p>
      <p>Research Scientist, Massachusetts Institute of Technology</p>
    </div>
  </div>

  <div class="w3-row w3-light-grey" style="margin-top:10px">
    <div class="w3-col" style="width:150px">
      <img src="./meta-NLP_files/daniel.jpg" onclick="onClick(this)" alt="Shang-Wen Li is a senior Applied Scientist at Amazon AI. His research focuses on spoken language understanding, dialog management, and natural language generation. His recent interest is transfer learning for low-resourced conversational bots. He earned his PhD from MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) in 2016. He received M.S. and B.S. from National Taiwan University. Before joining Amazon, he also worked at Apple Siri researching conversational AI. He is the workshop co-organizer about &quotSelf-Supervised Learning for Speech and Audio Processing&quot at NeurIPS (2020) and one of the tutorial speakers about &quotMeta Learning and its application to Human Language Processing&quot at Interspeech (2020)." class="w3-left w3-circle w3-margin-right w3-hover-opacity" style="width:120px; margin-top:25px;margin-bottom:25px;margin-left:8px">
    </div>
    <div class="w3-rest">
      <p><span class="w3-large w3-text-black w3-margin-right">Shang-Wen Li</span>
        </p>
        <button class="w3-button " onclick="window.open('https://swdanielli.github.io/', '_blank');"><i class="fa fa-user"></i></button>
        <button class="w3-button " onclick="window.location.href='mailto:shangwel@amazon.com';"><i class="fa fa-envelope"></i></button>
        </p>
      </p>
      <p>Senior Applied Scientist, Amzaon web services AI</p>
    </div>
  </div>

  <div class="w3-row" style="margin-top:10px">
    <div class="w3-col" style="width:150px">
      <img src="./meta-NLP_files/di.jpeg" onclick="onClick(this)" alt="Di Jin is an Applied Scientist at Amazon Alexa AI. His research focuses on conversational AI, adversarial robustness, transfer learning (esp. meta-learning for domain adaptation), question answering, and conditional text generation. He earned his PhD from MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) in 09/2020, supervised by Prof. Peter Szolovits. He received B.S. from Tsinghua University in China." class="w3-left w3-circle w3-margin-right w3-hover-opacity" style="width:120px; margin-top:25px;margin-bottom:25px;margin-left:8px">
    </div>
    <div class="w3-rest">
      <p><span class="w3-large w3-text-black w3-margin-right">Di Jin</span>
      <p><button class="w3-button" onclick="window.location.href='mailto:djinamzn@amazon.com';"><i class="fa fa-envelope"></i></button></p>
      </p>
      <p>Applied Scientist, Amazon Alexa AI</p>
    </div>
  </div>


  <div class="w3-row w3-light-grey" style="margin-top:10px">
    <div class="w3-col" style="width:150px">
      <img src="./meta-NLP_files/mandy.png" onclick="onClick(this)" alt="Mandy Korpusik is an Assistant Professor in the Department of Computer Science at Loyola Marymount University. She completed her B.S. in Electrical and Computer Engineering from Franklin W. Olin College of Engineering in May, 2013 and received her S.M. and PhD in Computer Science from MIT, where she worked for six years in the Spoken Language Systems group in the Computer Science and Artificial Intelligence Laboratory. Her primary research interests include natural language processing and spoken language understanding for dialogue systems. Mandy used deep learning models to build the Coco Nutritionist application for iOS that allows obesity patients to track their diet more easily through natural language. Her long-term research goal is to deploy a collection of AI-based conversational agents that improve the health, well-being, and productivity of people." class="w3-left w3-circle w3-margin-right w3-hover-opacity" style="width:120px; margin-top:25px;margin-bottom:25px;margin-left:8px">
    </div>
    <div class="w3-rest">
      <p><span class="w3-large w3-text-black w3-margin-right">Mandy Korpusik</span><p>
        <button class="w3-button w3-light-grey" onclick="window.open('http://people.csail.mit.edu/korpusik/', '_blank');"><i class="fa fa-user"></i> </button>
        <button class="w3-button w3-light-grey" onclick="window.location.href='mailto:mandy.korpusik@lmu.edu';"><i class="fa fa-envelope"></i></button></p>
      </p>
      <p>Assistant Professor, Loyola Marymount University</p>
    </div>
  </div>


  <div class="w3-row" style="margin-top:10px">
    <div class="w3-col" style="width:150px;">
      <img src="./meta-NLP_files/annie.png" onclick="onClick(this)" alt="Annie Dong is an Applied Scientist in Amazon Alexa AI working on data efficient and robustness strategies for Natural Language Understanding and Entity Resolution systems. Prior to joining Amazon, she had a couple short stints devising modeling solutions for healthcare tech, education tech, and finance applications. Annie received her M.S. from University of California, Santa Barbara." class="w3-left w3-circle w3-margin-right w3-hover-opacity" style="width:120px; margin-top:25px;margin-bottom:25px;margin-left:8px">
    </div>
    <div class="w3-rest">
      <p><span class="w3-large w3-text-black w3-margin-right">Annie Dong</span>
        <p><button class="w3-button" onclick="window.location.href='mailto:shuyand@amazon.com';"><i class="fa fa-envelope"></i></button>
      </p></p>
      <p>Applied Scientist, Amazon Alexa AI</p>
    </div>
  </div>


  <div class="w3-row w3-light-grey" style="margin-top:10px">
    <div class="w3-col" style="width:150px;">
      <img src="./meta-NLP_files/thang.jpg" onclick="onClick(this)" alt="Ngoc Thang Vu received his Diploma (2009) and PhD (2014) degrees in computer science from Karlsruhe Institute of Technology, Germany. From 2014 to 2015, he worked at Nuance Communications as a senior research scientist and at Ludwig-Maximilian University Munich as an acting professor in computational linguistics. In 2015, he was appointed assistant professor at University of Stuttgart, Germany. Since 2018, he has been a full professor at the Institute for Natural Language Processing in Stuttgart. His main research interests are natural language processing (esp. speech recognition and dialog systems) and machine learning (esp. deep learning) for low-resource settings. He is one of the tutorial speakers about &quotMeta Learning and its application to Human Language Processing&quot at Interspeech (2020)." class="w3-left w3-circle w3-margin-right w3-hover-opacity" style="width:120px; margin-top:25px;margin-bottom:25px;margin-left:8px">
    </div>
    <div class="w3-rest">
      <p><span class="w3-large w3-text-black w3-margin-right">Ngoc Thang Vu</span>
        <p>
        <button class="w3-button w3-light-grey" onclick="window.open('https://www.ims.uni-stuttgart.de/en/institute/team/Vu-00002/', '_blank');"><i class="fa fa-user"></i></button>
        <button class="w3-button w3-light-grey" onclick="window.location.href='mailto:ngoc-thang.vu@ims.uni-stuttgart.de';"><i class="fa fa-envelope"></i></button>
        </p>
      </p>
      <p>Professor, University of Stuttgart</p>
    </div>
  </div>


  <div class="w3-row" style="margin-top:10px">
    <div class="w3-col" style="width:150px">
      <img src="./meta-NLP_files/dilek.png" onclick="onClick(this)" alt="Dilek Hakkani-Tur is a senior principal scientist at Amazon Alexa AI focusing on enabling natural dialogues with machines. Prior to joining Amazon, she was leading the dialogue research group at Google Research, a principal researcher at Microsoft Research, International Computer Science Institute (ICSI) and AT&T Labs-Research. She received her PhD degree from Bilkent Univ., Department of Computer Engineering in 2000. Her research interests include conversational AI, natural language and speech processing, spoken dialogue systems, and machine learning for language processing. She has over 80 patents that were granted and co-authored more than 300 papers in natural language and speech processing. She is the recipient of three best paper awards for her work on active learning for dialogue systems, from IEEE Signal Processing Society, ISCA and EURASIP. She served as an associate editor for IEEE Transactions on Audio, Speech and Language Processing, member of the IEEE Speech and Language Technical Committee, area editor for speech and language processing for Elsevier's Digital Signal Processing Journal and IEEE Signal Processing Letters, and served on the ISCA Advisory Council. She is currently the Editor-in-Chief of the IEEE/ACM Transactions on Audio, Speech and Language Processing and a program co-chair for NAACL-HLT 2020. She is a fellow of the IEEE (2014) and ISCA (2014)." class="w3-left w3-circle w3-hover-opacity w3-margin-right" style="width:120px; margin-top:25px;margin-bottom:25px;margin-left:8px">
    </div>
    <div class="w3-rest">
      <p><span class="w3-large w3-text-black w3-margin-right">Dilek Hakkani-Tur</span>
        <p>
        <button class="w3-button" onclick="window.open('https://www.linkedin.com/in/dilek-hakkani-tur-9517543/', '_blank');"><i class="fa fa-user"></i></button>
        <button class="w3-button" onclick="window.location.href='mailto:hakkanit@amazon.com';"><i class="fa fa-envelope"></i></button>
        </p>
      </p>
      <p>Senior Principal Scientist, Amazon Alexa AI</p>
    </div>
  </div>
  </div>
</div>



<div class="w3-row-padding w3-padding-64 w3-container w3-light-grey" id="committee">
  <div class="w3-content">
      <h3 class="w3-center">Program committee</h3>
      <ul>
        <li> Trapit Bansal (University of Massachusetts, Amherst) </li>
        <li> Yangbin Chen (City University of Hong Kong) </li>
        <li> Yutian Chen (DeepMind) </li>
        <li> Samuel Coope (PolyAI) </li>
        <li> Jennifer Drexler (Rev) </li>
        <li> Tianyu Gao (Princeton University) </li>
        <li> Xavi Gonzalvo (Google Research) </li>
        <li> Yutai Hou (Harbin Institute of Technology) </li>
        <li> Kuan-Po Huang (National Taiwan University) </li>
        <li> Sathish Reddy Indurthi (Samsung) </li>
        <li> Ankit Jain (Facebook) </li>
        <li> Ping Jian (Beijing Institute of Technology) </li>
        <li> Tom Ko (Southern University of Science and Technology) </li>
        <li> Cheng-I Lai (Massachusetts Institute of Technology) </li>
        <li> Zhaojiang Lin (The Hong Kong University of Science and Technology) </li>
        <li> Yijia Liu (Alibaba Group) </li>
        <li> Colin Lockard (Amazon) </li>
        <li> Hongyin Luo (Massachusetts Institute of Technology) </li>
        <li> Meryem M'hamdi (University of Southern California) </li>
        <li> Andrea Madotto (The Hong Kong University of Science and Technology) </li>
        <li> Fei Mi (École Polytechnique Fédérale de Lausanne) </li>
        <li> Mehrad Moradshahi (Stanford University) </li>
        <li> Hoang Long Nguyen (Apple) </li>
        <li> Mohammad Salameh (University of Alberta) </li>
        <li> Hsuan Su (National Taiwan University) </li>
        <li> Jian Sun (Alibaba Group) </li>
        <li> Chenglong Wang (University of Washington) </li>
        <li> Yuan-Kuei Wu (National Taiwan University) </li>
        <li> Yu Zhang (Google Brain) </li>
      </ul>
  </div>
</div>

<!-- Modal for full size images on click-->
<div id="modal01" class="w3-modal w3-black" onclick="this.style.display='none'">
  <span class="w3-button w3-xxlarge w3-black w3-padding-large w3-display-topright" title="Close Modal Image">×</span>
  <div class="w3-modal-content w3-animate-zoom w3-center w3-transparent w3-padding-64">
    <img id="img01" class="w3-image">
    <p id="caption" class="w3-large"></p>
  </div>
</div>

<!-- Reading -->
<div class="w3-row-padding w3-padding-64 w3-container" id="reading">
  <div class="w3-content">
      <h3 class="w3-center">Reading</h3>
      <p>Meta learning is one of the fastest growing research areas in the deep learning scope. However there is no standard definition for meta learning. Usually the main goal is to design models that can learn new tasks rapidly with few in domain training examples, by having models to pre-learn from many, relevant or not, training tasks in a way that the models ar    e easy to be generalized to new tasks. For better understanding the scope of meta learning, we provide several online courses and papers describing the works falling into the area. These works are just for showcasing, and we definitely encourage people with research not covered here but sharing the same goal mentioned above to submit.
      </p>
      <h4> Online Courses </h4>
        <ul>
          <li>
            <a href="http://cs330.stanford.edu/" target="_blank" rel="noopener">CS 330: Deep Multi-Task and Meta Learning</a>
          </li>
          <li>
             <a href="https://youtu.be/wurPYalweeo" target="_blank" rel="noopener">Hung-Yi Lee’s Lecture</a> (in Mandarin)
          </li>  
        </ul>
      <h4 id="papers">Papers</h3>
        <h5 id="meta-learning-technology">Meta Learning Technology</h4>
          <ul>
            <li>Learning to Initialize:
              <ul>
                <li>Chelsea Finn, Pieter Abbeel, and Sergey Levine, “Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks”, ICML, 2017</li>
                <li>Sebastian Flennerhag, Pablo G. Moreno, Neil D. Lawrence, Andreas Damianou, Transferring Knowledge across Learning Processes, ICLR, 2019</li>
              </ul>
            </li>
            <li>Learning to optimize:
              <ul>
                <li>Sachin Ravi, Hugo Larochelle, Optimization as a model for few-shot learning, ICLR, 2017</li>
                <li>Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, Nando de Freitas, Learning to learn by gradient descent by gradient descent, NIPS, 2016</li>
              </ul>
            </li>
            <li>Learning to compare
              <ul>
                <li>Jake Snell, Kevin Swersky, Richard S. Zemel, Prototypical Networks for Few-shot Learning, NIPS, 2017</li>
                <li>Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra, Matching Networks for One Shot Learning, NIPS, 2016</li>
                <li>Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, Timothy M. Hospedales, Learning to Compare: Relation Network for Few-Shot Learning, CVPR, 2018</li>
              </ul>
            </li>
            <li>Learning the whole learning algorithm
              <ul>
                <li>Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy Lillicrap, Meta-Learning with Memory-Augmented Neural Networks, ICML, 2016</li>
                <li>Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, Pieter Abbeel, A Simple Neural Attentive Meta-Learner, ICLR, 2018</li>
              </ul>
            </li>
            <li>Network architecture search:
              <ul>
                <li>RL based
                  <ul>
                    <li>Barret Zoph, Quoc V. Le, Neural Architecture Search with Reinforcement Learning, ICLR 2017</li>
                    <li>Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le,  Learning Transferable Architectures for Scalable Image Recognition, CVPR, 2018</li>
                    <li>Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, Jeff Dean, Efficient Neural Architecture Search via Parameter Sharing, ICML, 2018</li>
                  </ul>
                </li>
                <li>Evolution based
                  <ul>
                    <li>Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc Le, Alex Kurakin, Large-Scale Evolution of Image Classifiers, ICML 2017</li>
                    <li>Esteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le, Regularized Evolution for Image Classifier Architecture Search, AAAI, 2019</li>
                    <li>Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray Kavukcuoglu, Hierarchical Representations for Efficient Architecture Search, ICLR, 2018</li>
                  </ul>
                </li>
                <li>Supernetwork based
                  <ul>
                    <li>Hanxiao Liu, Karen Simonyan, Yiming Yang, DARTS: Differentiable Architecture Search, ICLR, 2019</li>
                  </ul>
                </li>
              </ul>
            </li>
          </ul>
        <h5>Applications to Natural language understanding:</h4>

          <ul>
            <li><strong>[Bansal, et al., arXiv’19]</strong> Trapit Bansal, Rishikesh Jha, Andrew McCallum, Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks, arXiv, 2019</li>
            <li><strong>[Bose, et al., arXiv’19]</strong> Avishek Joey Bose, Ankit Jain, Piero Molino, William L. Hamilton, Meta-Graph: Few shot Link Prediction via Meta Learning, arXiv, 2019</li>
            <li><strong>[Campagna, et al., ACL’20]</strong> Giovanni Campagna, Agata Foryciarz, Mehrad Moradshahi, Monica Lam, Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking, ACL 2020</li>
            <li><strong>[Chen, et al., EMNLP’19]</strong> Mingyang Chen, Wen Zhang, Wei Zhang, Qiang Chen, Huajun Chen, Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs, EMNLP 2019</li>
            <li><strong>[Chen, et al., INTERSPEECH'20]</strong> Yi-Chen Chen, Jui-Yang Hsu, Cheng-Kuang Lee, Hung-yi Lee, DARTS-ASR: Differentiable architecture search for multilingual speech recognition and adaptation, INTERSPEECH 2020</li>
            <li><strong>[Chen, et al., ACL’20]</strong> Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, William Yang Wang, Few-Shot NLG with Pre-Trained Language Model, ACL 2020</li>
            <li><strong>[Chien, et al., INTERSPEECH’19]</strong> Jen-Tzung Chien, Wei Xiang Lieow, Meta Learning for Hyperparameter Optimization in Dialogue System, INTERSPEECH, 2019</li>
            <li><strong>[Coope, et al., ACL’20]</strong> Samuel Coope, Tyler Farghly, Daniela Gerz, Ivan Vulić, Matthew Henderson, Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations, ACL 2020</li>
            <li><strong>[Dou, et al., EMNLP’19]</strong> Zi-Yi Dou, Keyi Yu, Antonios Anastasopoulos, Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks, EMNLP 2019</li>
            <li><strong>[Geng, et al., EMNLP’19]</strong> Ruiying Geng, Binhua Li, Yongbin Li, Xiaodan Zhu, Ping Jian, Jian Sun, Induction Networks for Few-Shot Text Classification, EMNLP, 2019</li>
            <li><strong>[Geng, et al., ACL’20]</strong> Ruiying Geng, Binhua Li, Yongbin Li, Jian Sun, Xiaodan Zhu, Dynamic Memory Induction Networks for Few-Shot Text Classification, ACL 2020</li>
            <li><strong>[Gu, et al., EMNLP’18]</strong> Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, Victor O.K. Li, Meta-Learning for Low-Resource Neural Machine Translation, EMNLP, 2018</li>
            <li><strong>[Guo, et al., ACL’19]</strong> Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, Jian Yin, Coupling Retrieval and Meta-Learning for Context-Dependent Semantic Parsing, ACL, 2019</li>
            <li><strong>[Hou, et al., ACL’20]</strong> Yutai Hou, Wanxiang Che, Yongkui Lai, Zhihan Zhou, Yijia Liu, Han Liu, Ting Liu, Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network, ACL 2020</li>
            <li><strong>[Hu, et al., ACL’19]</strong> Ziniu Hu, Ting Chen, Kai-Wei Chang, Yizhou Sun, Few-Shot Representation Learning for Out-Of-Vocabulary Words, ACL 2019</li>
            <li><strong>[Huang, et al., NAACL’18]</strong> Po-Sen Huang, Chenglong Wang, Rishabh Singh, Wen-tau Yih, Xiaodong He, Natural Language to Structured Query Generation via Meta-Learning, NAACL 2018</li>
            <li><strong>[Huang, et al., ACL’20]</strong> Yi Huang, Junlan Feng, Min Hu, Xiaoting Wu, Xiaoyu Du, Shuo Ma, Meta-Reinforced Multi-Domain State Generator for Dialogue Systems, ACL 2020</li>
            <li><strong>[Kim, et al., ACL’20]</strong> Hwichan Kim, Tosho Hirasawa, Mamoru Komachi, Zero-shot North Korean to English Neural Machine Translation by Character Tokenization and Phoneme Decomposition, ACL 2020</li>
            <li><strong>[Luo, et al., INTERSPEECH'20]</strong> Hongyin Luo, Shang-Wen Li, James Glass, Prototypical q networks for automatic conversational diagnosis and few-shot new disease adaption, INTERSPEECH 2020</li>
            <li><strong>[Lv, et al., EMNLP’19]</strong> Xin Lv, Yuxian Gu, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu, Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot Relations, EMNLP 2019</li>
            <li><strong>[Madotto, et al., ACL’19]</strong> Andrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, Pascale Fung, Personalizing Dialogue Agents via Meta-Learning, ACL 2019</li>
            <li><strong>[Mohtarami, et al., EMNLP’19]</strong> Mitra Mohtarami, James Glass, Preslav Nakov, Contrastive language adaptation for cross-lingual stance detection, EMNLP 2019</li>
            <li><strong>[Mu, et al., ACL’20]</strong> Jesse Mu, Percy Liang, Noah Goodman, Shaping Visual Representations with Language for Few-shot Classification, ACL 2020</li>
            <li><strong>[Obamuyide, et al., ACL’19]</strong> Abiola Obamuyide, Andreas Vlachos, Model-Agnostic Meta-Learning for Relation Classification with Limited Supervision, ACL 2019</li>
            <li><strong>[Qian, et al., ACL’19]</strong> Kun Qian, Zhou Yu, Domain Adaptive Dialog Generation via Meta Learning, ACL 2019</li>
            <li><strong>[Shah, et al., ACL’19]</strong> Darsh Shah, Raghav Gupta, Amir Fayazi, Dilek Hakkani-Tur, Robust zero-shot cross-domain slot filling with example values, ACL 2019</li>
            <li><strong>[Song, et al., ACL’20]</strong> Yiping Song, Zequn Liu, Wei Bi, Rui Yan, Ming Zhang, Learning to Customize Model Structures for Few-shot Dialogue Generation Tasks, ACL 2020</li>
            <li><strong>[Sun, et al., EMNLP’18]</strong> Jingyuan Sun, Shaonan Wang, Chengqing Zong, Memory, Show the Way: Memory Based Few Shot Word Representation Learning, EMNLP 2018</li>
            <li><strong>[Sun, et al., EMNLP’19]</strong> Shengli Sun, Qingfeng Sun, Kevin Zhou, Tengchao Lv, Hierarchical Attention Prototypical Networks for Few-Shot Text Classification, EMNLP 2019</li>
            <li><strong>[Surís, et al., arXiv’19]</strong> Dídac Surís, Dave Epstein, Heng Ji, Shih-Fu Chang, Carl Vondrick, Learning to Learn Words from Narrated Video, arXiv, 2019</li>
            <li><strong>[Tan, et al., EMNLP’19]</strong> Ming Tan, Yang Yu, Haoyu Wang, Dakuo Wang, Saloni Potdar, Shiyu Chang, Mo Yu, Out-of-Domain Detection for Low-Resource Text Classification Tasks, EMNLP 2019</li>
            <li><strong>[Wang, et al., EMNLP’19]</strong> Zihao Wang, Kwun Ping Lai, Piji Li, Lidong Bing, Wai Lam, Tackling Long-Tailed Relations and Uncommon Entities in Knowledge Graph Completion, EMNLP 2019</li>
            <li><strong>[Winata, et al., ACL’20]</strong> Genta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, Peng Xu, Pascale Fung, Meta-Transfer Learning for Code-Switched Speech Recognition, ACL 2020</li>
            <li><strong>[Wu, et al., EMNLP’19]</strong> Jiawei Wu, Wenhan Xiong, William Yang Wang, Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label Classification, EMNLP 2019</li>
            <li><strong>[Wu, et al., AAAI’20]</strong> Qianhui Wu, Zijia Lin, Guoxin Wang, Hui Chen, Börje F. Karlsson, Biqing Huang, Chin-Yew Lin, Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with Minimal Resources, AAAI 2020</li>
            <li><strong>[Xiong, et al., EMNLP’18]</strong> Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, William Yang Wang, One-Shot Relational Learning for Knowledge Graphs, EMNLP 2018</li>
            <li><strong>[Yan, et al., ACL’20]</strong> Guangfeng Yan, Lu Fan, Qimai Li, Han Liu, Xiaotong Zhang, Xiao-Ming Wu, Albert Y.S. Lam, Unknown Intent Detection Using Gaussian Mixture Model with an Application to Zero-shot Intent Classification, ACL 2020</li>
            <li><strong>[Yan, et al., ACL’20]</strong> Ming Yan, Hao Zhang, Di Jin, Joey Tianyi Zhou, Multi-source meta transfer for low resource multiple-choice question answering, ACL 2020</li>
            <li><strong>[Ye, et al., ACL’20]</strong> Zhiquan Ye, Yuxia Geng, Jiaoyan Chen, Jingmin Chen, Xiaoxiao Xu, SuHang Zheng, Feng Wang, Jun Zhang, Huajun Chen, Zero-shot Text Classification via Reinforced Self-training, ACL 2020</li>
            <li><strong>[Ye, et al., ACL’19]</strong> Zhi-Xiu Ye, Zhen-Hua Ling, Multi-Level Matching and Aggregation Network for Few-Shot Relation Classification, ACL 2019</li>
            <li><strong>[Yu, et al., ACL’20]</strong> Changlong Yu, Jialong Han, Haisong Zhang, Wilfred Ng, Hypernymy Detection for Low-Resource Languages via Meta Learning, ACL 2020</li>
            <li><strong>[Yu, et al., ACL’18]</strong> Mo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni Potdar, Yu Cheng, Gerald Tesauro, Haoyu Wang, Bowen Zhou, Diverse Few-Shot Text Classification with Multiple Metrics, ACL 2018</li>
            <li><strong>[Zhang, et al., ACL’20]</strong> Biao Zhang, Philip Williams, Ivan Titov, Rico Sennrich, Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation, ACL 2020</li>
            <li><strong>[Zhao, et al. EMNLP’19]</strong> Zhenjie Zhao, Xiaojuan Ma, Text Emotion Distribution Learning from Small Sample: A Meta-Learning Approach, EMNLP 2019</li>
          </ul>
  </div>
</div>


<!-- Contact Section -->
<div class="w3-row-padding w3-padding-64 w3-container w3-light-grey" id="contact">
  <div class="w3-content">
    <h3 class="w3-center">CONTACT</h3>
    <p class="w3-center w3-large">Feel free to send us messages at:</p>
    <div class="w3-center" style="margin-top:20px">
      <p><i class="fa fa-envelope fa-fw w3-xlarge"> </i> Email: <a href="mailto:meta.nlp.acl.2021@gmail.com">meta.nlp.acl.2021@gmail.com</a></p>
      <br>
    </div>
  </div>  
</div>

<!-- Footer -->
<footer class="w3-center w3-black w3-padding-64">
  <a href="#home" class="w3-button w3-light-grey"><i class="fa fa-arrow-up w3-margin-right"></i>To the top</a>
  <p>Powered by <a href="https://www.w3schools.com/w3css/default.asp" title="W3.CSS" target="_blank" class="w3-hover-text-green">w3.css</a></p>
</footer>
 
<script>
// Modal Image Gallery
function onClick(element) {
  document.getElementById("img01").src = element.src;
  document.getElementById("modal01").style.display = "block";
  var captionText = document.getElementById("caption");
  captionText.innerHTML = element.alt;
}


// Toggle between showing and hiding the sidebar when clicking the menu icon
var mySidebar = document.getElementById("mySidebar");

function w3_open() {
  if (mySidebar.style.display === 'block') {
    mySidebar.style.display = 'none';
  } else {
    mySidebar.style.display = 'block';
  }
}

// Close the sidebar with the close button
function w3_close() {
    mySidebar.style.display = "none";
}
</script>

</body>
</html>

